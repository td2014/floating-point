{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Floating Point Issues in Computation\n",
    "## This notebook is a walkthrough and tutorial of floating point related topics, largely inspired (and closely following) the paper described below:\n",
    "\n",
    "Link: http://www.validlab.com/goldberg/paper.ps Summary from the paper \"Note – This document is an edited reprint of the paper What Every Computer Scientist Should Know About Floating-Point Arithmetic, by David Goldberg, published in the March, 1991 issue of Computing Surveys. Copyright 1991, Association for Computing Machinery, Inc., reprinted by permission.\"\n",
    "\n",
    "More details available from:\n",
    "http://grouper.ieee.org/groups/754/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The notebook organization will largely mirror the scheme from the above paper\n",
    "- Motivation\n",
    "- Part I: Rounding Errors\n",
    "- Part II: IEEE 754 Standard\n",
    "- Part III: Details (and other issues)\n",
    "- Addendum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "As implied in the title of the key reference \"What Every Computer Scientist Should Know About Floating-Point Arithmetic\" having a working understanding about this topic seems to be fundamental to essentially anyone working with computation.  Even though the details might be somewhat hidden inside the hardware and software implementations, the issues should be understood since they can manifest in various ways in practice.  This notebook attempts to extract key ideas from the reference(s) and provide some exercises to gain some familiarity with the points raised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Rounding Errors\n",
    "Rounding Error:\n",
    "Basic problem.  Infinite number of real numbers, finite number of bits.\n",
    "\n",
    "Hierarchy:  Real number → Floating point number → Encoding.\n",
    "\n",
    "Floating point numbers are certain real numbers that can be represented in a defined format (see below).  A given representation (defined by a base/radix, and exponent, and a sign) will provide for a particular set of real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic representation \n",
    "\n",
    "$ \\pm d_0.d_1 d_2 d_3 d_4 d_{p-1} \\times \\beta^e$ represents the number\n",
    "$ \\pm ( d_0+d_1 \\beta^{-1} + \\ldots + d_{p-1} \\beta^{-(p-1)} ) \\beta^e $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Show that the number of bits required to encode the above number is\n",
    "$ \\lceil log_{2} (e_{max}-e_{min}+1) \\rceil + \\lceil log_{2}\\beta^{p} \\rceil + 1 $  \n",
    "where $\\lceil x \\rceil$ is the ceiling function applied to $x$, $e_{min}$ is the minimum exponent, $e_{max}$ is the maximum exponent, $\\beta$ is the base (like 2 or 10), and $p$ is the number of places we are using to represent numbers in our system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response:\n",
    "\n",
    "**This section is an intentional small break in the text.  You can use it to type in your response to the exercise if you want.  I placed this buffer before each solution that I worked out, so you can hide my solution and work on your own first before comparing.  The solution section appears immediately below.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### End response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We require that $2^{N} > R$, where $N$ is the number of bit we require and $R$ is the real number we are trying to represent using binary.  \n",
    "Taking the base 2 logarithm of both sides yields:\n",
    "$N > log_2 R$.  If we take the ceiling of the right hand side \n",
    "(the smallest integer such that the inequality is satisfied)\n",
    "we obtain the desired result shown in the theory.  The additional 1 comes from the bit needed for representing the sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bits_exponent =  4.0\n",
      "num_bits_significand =  14.0\n",
      "num_bits_sign =  1\n",
      "total_bits =  19.0\n"
     ]
    }
   ],
   "source": [
    "# Exploration code:\n",
    "# Example of bits required\n",
    "import numpy as np\n",
    "emax =  5  \n",
    "emin = -4\n",
    "beta = 10 # base10\n",
    "p = 4     # four figures\n",
    "\n",
    "#TODO:  Implement the theory into the missing python code\n",
    "num_bits_exponent = np.ceil(np.log2(emax-emin+1))\n",
    "num_bits_significand = np.ceil(np.log2(beta**p))\n",
    "num_bits_sign = 1\n",
    "total_bits = num_bits_exponent + num_bits_significand + num_bits_sign\n",
    "\n",
    "print('num_bits_exponent = ', num_bits_exponent)\n",
    "print('num_bits_significand = ', num_bits_significand)\n",
    "print('num_bits_sign = ', num_bits_sign)\n",
    "print('total_bits = ', total_bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Representation\n",
    "When the number starts with a 1, this is called \"normalized.\"  That is to say that the $d_{0}$ in the above discussion of representation is set to 1.  It should be mentioned that in certain representations, this \"1\" is implied, but for now, let us assume it is required to be present (i.e. one bit in a memory location is used for this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Show that if $\\beta=2$, p=3, $e_{min}=-1$ and $e_{max}=2$, there are 16 normalized numbers that can be represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### End response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "For p = 3:  \n",
    "100  \n",
    "101  \n",
    "110  \n",
    "111  \n",
    "There is a group of these for each exponent:  $\\beta^{-1}$, $\\beta^{0}$, $\\beta^{1}$, and $\\beta^{2}$.\n",
    "Therefore we have 4x4 = 16 possible normalized numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Given  that the representation:\n",
    "$ \\pm d_0.d_1 d_2 d_3 d_4 d_{p-1} \\times \\beta^e$ represents the number\n",
    "$ \\pm ( d_0+d_1 \\beta^{-1} + \\ldots + d_{p-1} \\beta^{-(p-1)} ) \\beta^e $\n",
    "\n",
    "Write down what each of the 16 normalized encoding represent in decimal numbers (assume all positive).  Recall that for decimal (base 10), we would have a representation like this for the number 120\n",
    "\n",
    "$1.20 \\times 10^2 = 120 = 1 \\times 10^0(10^2) + 2 \\times 10^{-1}(10^2) + 0 \\times 10^{-2}(10^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### End response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "First two are done explicitly, remaining are computed using the Python code below.  As a side note, observe that the smallest number that can be represented in this case is 0.5.  During program execution, an exception called \"Underflow\" is triggered when a computation yields a result that will fall into the range of numbers less than 0.5.\n",
    "\n",
    "(exponent = -1.  Encoded as 00)  \n",
    "100 : 00  = $1 \\times 2^{0} 2^{-1} + 0 \\times 2^{-1} 2^{-1} + 0 \\times 2^{-2} 2^{-1} = 0.5$  \n",
    "101 : 00  = $1 \\times 2^{0} 2^{-1} + 0 \\times 2^{-1} 2^{-1} + 1 \\times 2^{-2} 2^{-1} = 0.5 + 0.125 = 0.625$  \n",
    "110 : 00  \n",
    "111 : 00  \n",
    "\n",
    "(exponent = 0.  Encoded as 01)  \n",
    "100 : 01   =   \n",
    "101 : 01  \n",
    "110 : 01  \n",
    "111 : 01  \n",
    "\n",
    "(exponent = 0.  Encoded as 10)  \n",
    "100 : 10  \n",
    "101 : 10  \n",
    "110 : 10  \n",
    "111 : 10  \n",
    "\n",
    "(exponent = 1.  Encoded as 11)  \n",
    "100 : 11  \n",
    "101 : 11  \n",
    "110 : 11  \n",
    "111 : 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.625\n",
      "0.75\n",
      "0.875\n",
      "\n",
      "1.0\n",
      "1.25\n",
      "1.5\n",
      "1.75\n",
      "\n",
      "2.0\n",
      "2.5\n",
      "3.0\n",
      "3.5\n",
      "\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise for normalized numbers\n",
    "import numpy as np\n",
    "\n",
    "# We want to generate the decimal equivalent of the above encodings:\n",
    "def normalized2decimal(prefix, exponent, base):\n",
    "    \n",
    "    res = 0\n",
    "    pos = 0\n",
    "    for iSym in prefix:\n",
    "        res = res+ 1.0*int(iSym)*(base**pos)*base**(exponent)\n",
    "        pos = pos-1\n",
    "        \n",
    "    return res\n",
    "\n",
    "# Try out some cases:\n",
    "\n",
    "print(normalized2decimal('100',-1,2))\n",
    "print(normalized2decimal('101',-1,2))\n",
    "print(normalized2decimal('110',-1,2))\n",
    "print(normalized2decimal('111',-1,2))\n",
    "print()\n",
    "print(normalized2decimal('100',0,2))\n",
    "print(normalized2decimal('101',0,2))\n",
    "print(normalized2decimal('110',0,2))\n",
    "print(normalized2decimal('111',0,2))\n",
    "print()\n",
    "print(normalized2decimal('100',1,2))\n",
    "print(normalized2decimal('101',1,2))\n",
    "print(normalized2decimal('110',1,2))\n",
    "print(normalized2decimal('111',1,2))\n",
    "print()\n",
    "print(normalized2decimal('100',2,2))\n",
    "print(normalized2decimal('101',2,2))\n",
    "print(normalized2decimal('110',2,2))\n",
    "print(normalized2decimal('111',2,2))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the normalized representation, the number zero has a special challenge because of the requirement to have a leading one.  Therefore, a special encoding is used.  One of the exponents is reserved to represent (or indicate) the number zero.  Therefore, continuing the earlier representation scheme, here is the complete list of 16 normalized numbers with zero included.  The exponent bits are shown to the right of the colon symbol below.  Notice that the exponent range is reduced by one to accomodate the encoding for zero. \n",
    "\n",
    "(special zero exponent = -2.  Encoded as 00)    \n",
    "100 : 00  = Zero    \n",
    "101 : 00  = Not valid  \n",
    "110 : 00  = Not valid  \n",
    "111 : 00  = Not valid  \n",
    "\n",
    "(exponent = -1. Encoded as 01)  \n",
    "100 : 01  = $1 \\times 2^{0} 2^{-1} + 0 \\times 2^{-1} 2^{-1} + 0 \\times 2^{-2} 2^{-1} = 0.5$    \n",
    "101 : 01  = $1 \\times 2^{0} 2^{-1} + 0 \\times 2^{-1} 2^{-1} + 1 \\times 2^{-2} 2^{-1} = 0.5 + 0.125 = 0.625$     \n",
    "110 : 01  \n",
    "111 : 01  \n",
    "\n",
    "(exponent = 0. Encoded as 10)  \n",
    "100 : 10 =  \n",
    "101 : 10  \n",
    "110 : 10  \n",
    "111 : 10  \n",
    "\n",
    "(exponent = 1. Encoded as 11)  \n",
    "100 : 11 =  \n",
    "101 : 11  \n",
    "110 : 11  \n",
    "111 : 11  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Error, Units in the Last Place (ULPs), Machine Epsilon\n",
    "\n",
    "At this point, the general theme should be emerging in that an arbitrary real number (\"infinite precision\") is approximated in the computed by a floating point representation.  There are gaps between each floating point representation when compared to the real-numbers, such that a given real number may fall between two floating point representations and thus, will need to either be assigned to the lower one or the upper one.  Thus, the error is bounded to be no more than $\\frac{1}{2}$ of the spacing at that particular interval, which from the above analysis is clearly not uniform, but depends on the exponent that applies in that particular range.\n",
    "\n",
    "There is an ultimate limit to this, which is known as *machine epsilon*, $\\epsilon$, which is the smallest interval that can be represented by the machine implementation.\n",
    "\n",
    "The *units in the last place* (ulps) is absolute error between the real number in question and the closest floating point number.  For example, consider 0.03412 as the real number, and 0.034 as the floating point representation (assuming for a moment this number can be represented as shown).  Then we have $0.03412-0.034=0.12ulps$.\n",
    "\n",
    "*Relative error* is this difference, divided by the real number itself.  So in this case, we would have $\\frac{0.00012}{0.03412}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancellation Effects\n",
    "\n",
    "One of the main areas that need to be understood is the limitations of floating-point representations for certain types of computations, and strategies to remedy the impact.\n",
    "\n",
    "Basically, without appropriate measures, subtractions between two close numbers can produce large relative errors.  We'll go through several examples, and then explain some remedies.  These remedies may be \"built-in\" to the processing logic or may required interventional by the developer.  However, it is well-worth understanding the issues and potential traps to avoid.\n",
    "\n",
    "The main culprit is the following.  In any sequence of calculations, you may introduce round off errors.  If you end up with a calculation that subtracts two terms, each of which have round-off errors, then the result will be significantly different than the \"infinite precision\" version of the calculation.  The main point to keep in mind is that this only an issue when you have round off errors (i.e. your intermediate result was mapped to the nearest number that could be represented in floating point.).\n",
    "\n",
    "To illustrate the second type of cancellation first, let us assume we have two exactly representable numbers using base-2 for convenience and use the above normalized sequence as our represent out computations.  The difference between 1.5 and 1.0 = 0.5.  This is an exactly representable number, so there is no issue.  However, take the case of the difference between 1.6 and 1.0 = 0.6 in exact terms.  Now, 1.6 will be rounded to 1.5, and the difference taken will be 1.5-1.0 = 0.5.  This is an error of $\\frac{0.6-0.5}{0.6}\\approx16.7\\%$.  \n",
    "\n",
    "This is bad, but the absolute worst possible result is if the two terms round to the same floating point number, say we were computing 1.5-1.4.  The real answer is 0.1, but the result after 1.4 is rounded to 1.5 is *ZERO*.  Clearly, the use of this intermediate result in a downstream calculation would be highly suspect if not \"catastrophic\" as described in the Goldberg reference mentioned in the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact vs. Approximate Calculations\n",
    "\n",
    "Up to this point, we haven't emphasized the subtleties of exact vs. approximate calculations.  Indeed, this is a complete subject in its own right called *Numerical Analysis*.  However, some key distinctions need to be made.  We should always keep in mind that there is some sort of exact calculation we are attempting to reproduce using the computer program/algorithm.  In fact, there is a further level to this to consider, namely how one actually computes a function in the first place even if you have infinite precision available?  For example, let $f(x)=\\sqrt{x}$ be the function we wish to compute, for $x\\ge0$.  Here is one possible approach we could take:\n",
    "\n",
    "1) Intialize m = 0  \n",
    "2) Compute res_estimate_low_2 = m\\*m  \n",
    "3) Compute res_estimate_high_2 = (m+1)\\*(m+1)  \n",
    "4) If res_estimate_low_2 = x then return m    \n",
    "5) If res_estimate_low_2 < x and res_estimate_high_2 > x then return m+1  \n",
    "6) Else m=m+1, repeat from step (2).  \n",
    "\n",
    "The above algorithm can be done by hand and on machines with finite precision without error assuming representing the intermediate calculations do not exceed the memory for representing a given number.  However, the result may not be sufficiently close to the exact answer for the problem at hand.  How an algorithm goes about generating a more accurate answer will be related to the matter of rounding schemes.\n",
    "\n",
    "As a precursor to this discussion, we can consider a situation whereby a multiplication operation which starts with two exactly representable numbers, can lead to a result which is not exactly representable in the given architecture.  For example, in the normalized representation we gave earlier, we have two exactly represented numbers, 1.75 and 2.5 which when multiplied together produce 4.375.  As we can see from the above results, this is not one of the exactly representable numbers in that architecture, and thus would introduce an error in that the multiplication algorithm would need to somehow assign the answer to either 4.0 or 5.0.  No matter what choice is made, a fairly sizeable error is introduced compared to the \"exact\" computation.  Again, this may or may not be significant based on the requirements of the specific problem at hand - however, understanding and managing this issue is paramount in any practical general purpose computing environment.\n",
    "\n",
    "To illustrate some of the design considerations, let us simulate a hardware multiplier and then compare with a software multiplier implementation that is compliant with a specific requirement (IEEE754)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multiplication algorithm in hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multipliction algorithm in software compliant with IEEE 754"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be even more specific, only symbolic calculations can be truly \"exact\" in the way we are referencing it in this discussion.  In other words, we obtain a formula or expression using mathematical rules from a starting point to a given result, which is in terms of symbols, not actual numbers.  At some point, an interpreter or parser will process the given symbolic expression and convert into operations that can be implemented in some machine.  It is at this stage that the potential for errors is introduced as a consequence of finite precision arithmetic in any practical implementation (i.e. time and space constrained).  A particular input is provided which will be subject to transformation such it can be represented in the machine, a sequence of operations, and then a final result.\n",
    "\n",
    "Therefore, to be clear, let us use the following designations:  A *true* result is obtained by symbolic manipulation with no error, an *exact* result is one where the operands are not subject to being rounded by a previous stage, and final a *computed* result is one where the operands themselves have been subject to rounding.  It should be pointed out that this discussion is referring primarily to the intermediate results of a sequence of steps in a calculation.  The inputs to the formula may themselves be subject to transformation if they are not exactly representable as floating point numbers in the machine architecture, so in that case we are dealing with a *computed* result with its attendant error analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guard Digits\n",
    "\n",
    "We now begin some discussion about how these issues are addressed in practice.\n",
    "\n",
    "\n",
    "## Exactly Rounded Operations\n",
    "\n",
    "This section discusses other approaches and consequences to floating-point issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: IEEE 754\n",
    "\n",
    "Discussion of the floating point standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Proof Details (and other issues)\n",
    "\n",
    "Proof details, and other issues.\n",
    "\n",
    "## Rounding Error Related Proofs\n",
    "\n",
    "## Binary to Decimal Conversion Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addendum\n",
    "\n",
    "Discussions about various implementations, further topics, and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A =  [0, 0, 1, 1]\n",
      "B =  [0, 1, 0, 1]\n",
      "----\n",
      "result A>B =  [0, 0, 1, 0]\n",
      "----\n",
      "result A<B =  [0, 1, 0, 0]\n",
      "----\n",
      "\n",
      "newRes (A overall > B) =  1\n"
     ]
    }
   ],
   "source": [
    "# Implementing a comparator using Boolean Logic Gates\n",
    "# This is just an exploratory exercise\n",
    "# \n",
    "# We will consider 4 bit words for this, although this can be generalized\n",
    "#\n",
    "\n",
    "# Basic Assumed Fact:\n",
    "# If one bit position has A<B, then there must be \n",
    "# at least one bit position at higher order \n",
    "# (i.e. higher bit index, assuming lowest order bit is numbered 0) that has A>B, \n",
    "# otherwise the overall A>B is false.\n",
    "#\n",
    "\n",
    "#\n",
    "# Create a function that implements the following truth table:  \n",
    "#\n",
    "#    X>Y:\n",
    "#\n",
    "#    X   Y  output\n",
    "#    -------------\n",
    "#    0   0    0\n",
    "#    0   1    0\n",
    "#    1   0    1\n",
    "#    1   1    0\n",
    "#\n",
    "#  -->  X AND (!Y)\n",
    "#\n",
    "\n",
    "def x_gt_y_gate(x,y):\n",
    "    \n",
    "    if len(x)!=len(y):\n",
    "        raise ValueError('Error.  Argument lists must have same length.')\n",
    "        \n",
    "    z = []\n",
    "    for x_curr, y_curr in zip(x,y):\n",
    "        z_curr = x_curr and (not y_curr)\n",
    "        z.append(int(z_curr))\n",
    "        \n",
    "    return z\n",
    "\n",
    "A = [0,0,1,1]  # Note, far left is low order bit (A[0])\n",
    "B = [0,1,0,1]\n",
    "\n",
    "print('A = ', A)\n",
    "print('B = ', B)\n",
    "\n",
    "upper = x_gt_y_gate(A,B)\n",
    "print('----')\n",
    "print('result A>B = ', upper)\n",
    "\n",
    "#\n",
    "# Create a function that implements the following truth table:  \n",
    "#\n",
    "#    X<Y:\n",
    "#\n",
    "#    X   Y  output\n",
    "#    -------------\n",
    "#    0   0    0\n",
    "#    0   1    1\n",
    "#    1   0    0\n",
    "#    1   1    0\n",
    "#\n",
    "#  -->  !X AND Y\n",
    "#\n",
    "\n",
    "def x_lt_y_gate(x,y):\n",
    "    \n",
    "    if len(x)!=len(y):\n",
    "        raise ValueError('Error.  Argument lists must have same length.')\n",
    "        \n",
    "    z = []\n",
    "    for x_curr, y_curr in zip(x,y):\n",
    "        z_curr = (not x_curr) and y_curr\n",
    "        z.append(int(z_curr))\n",
    "        \n",
    "    return z\n",
    "\n",
    "##A = [0,0,1,1]  # Note, far left is low order bit (A[0])\n",
    "##B = [0,1,0,1]\n",
    "\n",
    "lower = x_lt_y_gate(A,B)\n",
    "print('----')\n",
    "print('result A<B = ', lower)\n",
    "\n",
    "#\n",
    "# Now create priority validator to implement the Basic Assumed Fact:\n",
    "# If one bit position has A<B, then there must be \n",
    "# at least one bit position at higher order \n",
    "# (i.e. higher bit index, assuming lowest order bit is numbered 0) that has A>B, \n",
    "# otherwise the overall A>B is false.\n",
    "#\n",
    "\n",
    "def pri_valid_4(upper, lower):\n",
    "    \n",
    "    # upper is true for bits in A where A>B\n",
    "    # lower is true for bits in A where A<B\n",
    "    # if both are false, then that bit position matches\n",
    "    \n",
    "    if len(upper) != 4 or len(lower) !=4:\n",
    "        raise ValueError('Error.  Only 4-bit words allowed.')\n",
    "        \n",
    "#\n",
    "# Implement priority logic\n",
    "#\n",
    "\n",
    "    output_3 = not lower[3]  # if this is true, A !>B\n",
    "    output_2 =  (upper[3] and lower[2]) or not lower[2]\n",
    "    output_1 = ((upper[3] or upper[2]) and lower[1]) or not lower[1]\n",
    "    output_0 = ((upper[3] or upper[2] or upper[1]) and lower[0]) or not lower[0]\n",
    "\n",
    "#\n",
    "# final output is the and of the intermediates\n",
    "#\n",
    "    \n",
    "    final_output = output_0 and output_1 and output_2 and output_3\n",
    "    \n",
    "#\n",
    "#   Trap for full equality in the bits\n",
    "#\n",
    "    \n",
    "    full_equal = upper[3] or upper[2] or upper[1] or upper[0] or lower[3] or lower[2] or lower[1] or lower[0]\n",
    "    final_output = final_output and full_equal  # if all are zero, then arrays are equal\n",
    "    \n",
    "    return int(final_output)\n",
    "\n",
    "#\n",
    "# Do a quick test\n",
    "#\n",
    "\n",
    "print(\"----\")\n",
    "print()\n",
    "\n",
    "##upper = [1,1,1,0]\n",
    "##lower = [0,0,0,1]\n",
    "\n",
    "newRes = pri_valid_4(upper,lower)\n",
    "print('newRes (A overall > B) = ', newRes)\n",
    "\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
