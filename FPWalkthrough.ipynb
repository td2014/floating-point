{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Floating Point Issues in Computation\n",
    "## This notebook is a walkthrough and tutorial of floating point related topics, largely inspired (and closely following) the paper described below:\n",
    "\n",
    "Link: http://www.validlab.com/goldberg/paper.ps Summary from the paper \"Note – This document is an edited reprint of the paper What Every Computer Scientist Should Know About Floating-Point Arithmetic, by David Goldberg, published in the March, 1991 issue of Computing Surveys. Copyright 1991, Association for Computing Machinery, Inc., reprinted by permission.\"\n",
    "\n",
    "More details available from:\n",
    "http://grouper.ieee.org/groups/754/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The notebook organization will largely mirror the scheme from the above paper\n",
    "- Motivation\n",
    "- Part I: Rounding Errors\n",
    "- Part II: IEEE 754 Standard\n",
    "- Part III: Details (and other issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "As implied in the title of the key reference \"What Every Computer Scientist Should Know About Floating-Point Arithmetic\" having a working understanding about this topic seems to be fundamental to essentially anyone working with computation.  Even though the details might be somewhat hidden inside the hardware and software implementations, the issues should be understood since they can manifest in various ways in practice.  This notebook attempts to extract key ideas from the reference(s) and provide some exercises to gain some familiarity with the points raised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Rounding Errors\n",
    "Rounding Error:\n",
    "Basic problem.  Infinite number of real numbers, finite number of bits.\n",
    "\n",
    "Real number → Representable by a floating point representation (but sometimes not exactly)\n",
    "When exact, it is called a floating point number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic representation \n",
    "\n",
    "$ \\pm d_0.d_1 d_2 d_3 d_4 d_{p-1} \\times \\beta^e$ represents the number\n",
    "$ \\pm ( d_0+d_1 \\beta^{-1} + \\ldots + d_{p-1} \\beta^{-(p-1)} ) \\beta^e $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Show that the number of bits required to encode the above number is\n",
    "$ \\lceil log_{2} (e_{max}-e_{min}+1) \\rceil + \\lceil log_{2}\\beta^{p} \\rceil + 1 $  \n",
    "where $\\lceil x \\rceil$ is the ceiling function applied to $x$, $e_{min}$ is the minimum exponent, $e_{max}$ is the maximum exponent, $\\beta$ is the base (like 2 or 10), and $p$ is the number of places we are using to represent numbers in our system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response:\n",
    "\n",
    "**This section is an intentional small break in the text.  You can use it to type in your response to the exercise if you want.  I placed this buffer before each solution that I worked out, so you can hide my solution and work on your own first before comparing.  The solution section appears immediately below.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### End response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We require that $2^{N} > R$, where $N$ is the number of bit we require and $R$ is the real number we are trying to represent using binary.  \n",
    "Taking the base 2 logarithm of both sides yields:\n",
    "$N > log_2 R$.  If we take the ceiling of the right hand side \n",
    "(the smallest integer such that the inequality is satisfied)\n",
    "we obtain the desired result shown in the theory.  The additional 1 comes from the bit needed for representing the sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bits_exponent =  4.0\n",
      "num_bits_significand =  14.0\n",
      "num_bits_sign =  1\n",
      "total_bits =  19.0\n"
     ]
    }
   ],
   "source": [
    "# Exploration code:\n",
    "# Example of bits required\n",
    "import numpy as np\n",
    "emax =  5  \n",
    "emin = -4\n",
    "beta = 10 # base10\n",
    "p = 4     # four figures\n",
    "\n",
    "#TODO:  Implement the theory into the missing python code\n",
    "num_bits_exponent = np.ceil(np.log2(emax-emin+1))\n",
    "num_bits_significand = np.ceil(np.log2(beta**p))\n",
    "num_bits_sign = 1\n",
    "total_bits = num_bits_exponent + num_bits_significand + num_bits_sign\n",
    "\n",
    "print('num_bits_exponent = ', num_bits_exponent)\n",
    "print('num_bits_significand = ', num_bits_significand)\n",
    "print('num_bits_sign = ', num_bits_sign)\n",
    "print('total_bits = ', total_bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Representation\n",
    "When the number starts with a 1, this is called \"normalized.\"  That is to say that the $d_{0}$ in the above discussion of representation is set to 1.  It should be mentioned that in certain representations, this \"1\" is implied, but for now, let us assume it is required to be present (i.e. one bit in a memory location is used for this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Show that if $\\beta=2$, p=3, $e_{min}=-1$ and $e_{max}=2$, there are 16 normalized numbers that can be represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### End response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "For p = 3:  \n",
    "100  \n",
    "101  \n",
    "110  \n",
    "111  \n",
    "There is a group of these for each exponent:  $\\beta^{-1}$, $\\beta^{0}$, $\\beta^{1}$, and $\\beta^{2}$.\n",
    "Therefore we have 4x4 = 16 possible normalized numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Given  that the representation:\n",
    "$ \\pm d_0.d_1 d_2 d_3 d_4 d_{p-1} \\times \\beta^e$ represents the number\n",
    "$ \\pm ( d_0+d_1 \\beta^{-1} + \\ldots + d_{p-1} \\beta^{-(p-1)} ) \\beta^e $\n",
    "\n",
    "Write down what each of the 16 normalized encoding represent in decimal numbers (assume all positive).  Recall that for decimal (base 10), we would have a representation like this for the number 120\n",
    "\n",
    "$1.20 \\times 10^2 = 120 = 1 \\times 10^0(10^2) + 2 \\times 10^{-1}(10^2) + 0 \\times 10^{-2}(10^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### End response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "First two are done explicitly, remaining are computed using the Python code below.  As a side note, observe that the smallest number that can be represented in this case is 0.5.  During program execution, an exception called \"Underflow\" is triggered when a computation yields a result that will fall into the range of numbers less than 0.5.\n",
    "\n",
    "(exponent = -1.  Encoded as 00)  \n",
    "100 : 00  = $1 \\times 2^{0} 2^{-1} + 0 \\times 2^{-1} 2^{-1} + 0 \\times 2^{-2} 2^{-1} = 0.5$  \n",
    "101 : 00  = $1 \\times 2^{0} 2^{-1} + 0 \\times 2^{-1} 2^{-1} + 1 \\times 2^{-2} 2^{-1} = 0.5 + 0.125 = 0.625$  \n",
    "110 : 00  \n",
    "111 : 00  \n",
    "\n",
    "(exponent = 0.  Encoded as 01)  \n",
    "100 : 01   =   \n",
    "101 : 01  \n",
    "110 : 01  \n",
    "111 : 01  \n",
    "\n",
    "(exponent = 0.  Encoded as 10)  \n",
    "100 : 10  \n",
    "101 : 10  \n",
    "110 : 10  \n",
    "111 : 10  \n",
    "\n",
    "(exponent = 1.  Encoded as 11)  \n",
    "100 : 11  \n",
    "101 : 11  \n",
    "110 : 11  \n",
    "111 : 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.625\n",
      "0.75\n",
      "0.875\n",
      "\n",
      "1.0\n",
      "1.25\n",
      "1.5\n",
      "1.75\n",
      "\n",
      "2.0\n",
      "2.5\n",
      "3.0\n",
      "3.5\n",
      "\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise for normalized numbers\n",
    "import numpy as np\n",
    "\n",
    "# We want to generate the decimal equivalent of the above encodings:\n",
    "def normalized2decimal(prefix, exponent, base):\n",
    "    \n",
    "    res = 0\n",
    "    pos = 0\n",
    "    for iSym in prefix:\n",
    "        res = res+ 1.0*int(iSym)*(base**pos)*base**(exponent)\n",
    "        pos = pos-1\n",
    "        \n",
    "    return res\n",
    "\n",
    "# Try out some cases:\n",
    "\n",
    "print(normalized2decimal('100',-1,2))\n",
    "print(normalized2decimal('101',-1,2))\n",
    "print(normalized2decimal('110',-1,2))\n",
    "print(normalized2decimal('111',-1,2))\n",
    "print()\n",
    "print(normalized2decimal('100',0,2))\n",
    "print(normalized2decimal('101',0,2))\n",
    "print(normalized2decimal('110',0,2))\n",
    "print(normalized2decimal('111',0,2))\n",
    "print()\n",
    "print(normalized2decimal('100',1,2))\n",
    "print(normalized2decimal('101',1,2))\n",
    "print(normalized2decimal('110',1,2))\n",
    "print(normalized2decimal('111',1,2))\n",
    "print()\n",
    "print(normalized2decimal('100',2,2))\n",
    "print(normalized2decimal('101',2,2))\n",
    "print(normalized2decimal('110',2,2))\n",
    "print(normalized2decimal('111',2,2))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the normalized representation, the number zero has a special challenge because of the requirement to have a leading one.  Therefore, a special encoding is used.  One of the exponents is reserved to represent (or indicate) the number zero.  Therefore, continuing the earlier representation scheme, here is the complete list of 16 normalized numbers with zero included.  The exponent bits are shown to the right of the colon symbol below.  Notice that the exponent range is reduced by one to accomodate the encoding for zero. \n",
    "\n",
    "(special zero exponent = -2.  Encoded as 00)    \n",
    "100 : 00  = Zero    \n",
    "101 : 00  = Not valid  \n",
    "110 : 00  = Not valid  \n",
    "111 : 00  = Not valid  \n",
    "\n",
    "(exponent = -1. Encoded as 01)  \n",
    "100 : 01  = $1 \\times 2^{0} 2^{-1} + 0 \\times 2^{-1} 2^{-1} + 0 \\times 2^{-2} 2^{-1} = 0.5$    \n",
    "101 : 01  = $1 \\times 2^{0} 2^{-1} + 0 \\times 2^{-1} 2^{-1} + 1 \\times 2^{-2} 2^{-1} = 0.5 + 0.125 = 0.625$     \n",
    "110 : 01  \n",
    "111 : 01  \n",
    "\n",
    "(exponent = 0. Encoded as 10)  \n",
    "100 : 10 =  \n",
    "101 : 10  \n",
    "110 : 10  \n",
    "111 : 10  \n",
    "\n",
    "(exponent = 1. Encoded as 11)  \n",
    "100 : 11 =  \n",
    "101 : 11  \n",
    "110 : 11  \n",
    "111 : 11  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Error, Units in the Last Place (ULPs), Machine Epsilon\n",
    "\n",
    "At this point, the general theme should be emerging in that an arbitrary real number (\"infinite precision\") is approximated in the computed by a floating point representation.  There are gaps between each floating point representation when compared to the real-numbers, such that a given real number may fall between two floating point representations and thus, will need to either be assigned to the lower one or the upper one.  Thus, the error is bounded to be no more than $\\frac{1}{2}$ of the spacing at that particular interval, which from the above analysis is clearly not uniform, but depends on the exponent that applies in that particular range.\n",
    "\n",
    "There is an ultimate limit to this, which is known as *machine epsilon*, $\\epsilon$, which is the smallest interval that can be represented by the machine implementation.\n",
    "\n",
    "The *units in the last place* (ulps) is absolute error between the real number in question and the closest floating point number.  For example, consider 0.03412 as the real number, and 0.034 as the floating point representation (assuming for a moment this number can be represented as shown).  Then we have $0.03412-0.034=0.12ulps$.\n",
    "\n",
    "*Relative error* is this difference, divided by the real number itself.  So in this case, we would have $\\frac{0.00012}{0.03412}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancellation Effects\n",
    "\n",
    "One of the main areas that need to be understood is the limitations of floating-point representations for certain types of computations, and strategies to remedy the impact.\n",
    "\n",
    "Basically, without appropriate measures, subtractions between two close numbers can produce large relative errors.  We'll go through several examples, and then explain some remedies.  These remedies may be \"built-in\" to the processing logic or may required interventional by the developer.  However, it is well-worth understanding the issues and potential traps to avoid.\n",
    "\n",
    "The main culprit is the following.  In any sequence of calculations, you may introduce round off errors.  If you end up with a calculation that subtracts two terms, each of which have round-off errors, then the result will be significantly different than the \"infinite precision\" version of the calculation.  The main point to keep in mind is that this only an issue when you have round off errors (i.e. your intermediate result was mapped to the nearest number that could be represented in floating point.).\n",
    "\n",
    "To illustrate the second type of cancellation first, let us assume we have two exactly representable numbers using base-2 for convenience and use the above normalized sequence as our represent out computations.  The difference between 1.5 and 1.0 = 0.5.  This is an exactly representable number, so there is no issue.  However, take the case of the difference between 1.6 and 1.0 = 0.6 in exact terms.  Now, 1.6 will be rounded to 1.5, and the difference taken will be 1.5-1.0 = 0.5.  This is an error of $\\frac{0.6-0.5}{0.6}\\approx16.7\\%$.  \n",
    "\n",
    "This is bad, but the absolute worst possible result is if the two terms round to the same floating point number, say we were computing 1.5-1.4.  The real answer is 0.1, but the result after 1.4 is rounded to 1.5 is *ZERO*.  Clearly, the use of this intermediate result in a downstream calculation would be highly suspect if not \"catastrophic\" as described in the Goldberg reference mentioned in the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact vs. Approximate Calculations\n",
    "\n",
    "Up to this point, we haven't emphasized the subtleties of exact vs. approximate calculations.  Indeed, this is a complete subject in its own right called *Numerical Analysis*.  However, some key distinctions need to be made.  We should always keep in mind that there is some sort of exact calculation we are attempting to reproduce using the computer program/algorithm.  In fact, there is a further level to this to consider, namely how one actually computes a function in the first place even if you have infinite precision available?  For example, let $f(x)=\\sqrt{x}$ be the function we wish to compute, for $x\\ge0$.  Here is one possible approach we could take:\n",
    "\n",
    "1) Intialize m = 0  \n",
    "2) Compute res_estimate_low_2 = m\\*m  \n",
    "3) Compute res_estimate_high_2 = (m+1)\\*(m+1)  \n",
    "4) If res_estimate_low_2 = x then return m    \n",
    "5) If res_estimate_low_2 < x and res_estimate_high_2 > x then return m+1/2  \n",
    "6) Else m=m+1, repeat from step (2).  \n",
    "\n",
    "The above algorithm can be done by hand and on machines with finite precision without error assuming representing the intermediate calculations do not exceed the memory for representing a given number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guard Digits\n",
    "\n",
    "We now begin some discussion about how these issues are addressed in practice.\n",
    "\n",
    "\n",
    "## Exactly Rounded Operations\n",
    "\n",
    "This section discusses other approaches and consequences to floating-point issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: IEEE 754\n",
    "\n",
    "Discussion of the floating point standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Details (and other issues)\n",
    "\n",
    "Proof details, and other issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
